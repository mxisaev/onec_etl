#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü—ã companyproducts
- –£–¥–∞–ª—è–µ—Ç –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏
- –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
- –°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –ø–µ—Ä–µ–¥ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
- –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º
"""

import sys
from pathlib import Path
import pandas as pd
from loguru import logger
import psycopg2
from dotenv import load_dotenv
import os
import json
from datetime import datetime

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ç–µ–∫—É—â–µ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent  # oneC_etl/
DOCKER_ROOT = PROJECT_ROOT.parent.parent  # docker/

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º –ø—Ä–æ–µ–∫—Ç–∞
sys.path.append(str(PROJECT_ROOT))

# –ò—â–µ–º .env —Ñ–∞–π–ª –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
env_paths = [
    PROJECT_ROOT / '.env',
    DOCKER_ROOT / 'dags' / '.env',
    DOCKER_ROOT / '.env',
    Path('/var/www/vhosts/itland.uk/docker/dags/.env')
]

env_loaded = False
for env_path in env_paths:
    if env_path.exists():
        load_dotenv(env_path)
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑: {env_path}")
        env_loaded = True
        break

if not env_loaded:
    logger.warning("–§–∞–π–ª .env –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")

def get_database_connection():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö"""
    try:
        conn = psycopg2.connect(
            host=os.getenv('POSTGRES_HOST', 'localhost'),
            port=os.getenv('POSTGRES_PORT', '5432'),
            dbname=os.getenv('POSTGRES_DB', 'data'),
            user=os.getenv('POSTGRES_USER', 'postgresadmin'),
            password=os.getenv('POSTGRES_PASSWORD', 'J5-unaxda3SK')
        )
        return conn
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

def get_table_structure(table_name):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã"""
    conn = get_database_connection()
    try:
        with conn.cursor() as cur:
            cur.execute("""
                SELECT column_name, data_type, is_nullable
                FROM information_schema.columns 
                WHERE table_schema = 'public' 
                AND table_name = %s
                ORDER BY ordinal_position;
            """, (table_name,))
            columns = cur.fetchall()
            return {col[0]: {'type': col[1], 'nullable': col[2]} for col in columns}
    finally:
        conn.close()

def get_required_columns():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
    return {
        'id': 'uuid',
        'description': 'text', 
        'brand': 'character varying',
        'category': 'character varying',
        'withdrawn_from_range': 'boolean',
        'item_number': 'character varying',
        'product_category': 'character varying',
        'on_order': 'boolean',
        'is_vector': 'boolean',
        'count_rows': 'integer',
        'merged': 'text',  # ‚úÖ –ö–æ–ª–æ–Ω–∫–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        'upload_timestamp': 'timestamp with time zone',
        'updated_at': 'timestamp with time zone',
        'vector': 'USER-DEFINED'  # pgvector —Ç–∏–ø
    }

def confirm_cleanup(columns_to_drop, current_structure):
    """–°–ø—Ä–æ—Å–∏—Ç—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    required_columns = get_required_columns()
    
    print("\n" + "="*80)
    print("üéØ –ü–õ–ê–ù –û–ß–ò–°–¢–ö–ò –¢–ê–ë–õ–ò–¶–´ COMPANYPRODUCTS")
    print("="*80)
    print(f"üìä –í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {len(columns_to_drop)}")
    print(f"üìä –í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è: {len(required_columns)}")
    
    print("\n‚ùå –ö–û–õ–û–ù–ö–ò –ö–û–¢–û–†–´–ï –ë–£–î–£–¢ –£–î–ê–õ–ï–ù–´:")
    for i, col in enumerate(columns_to_drop, 1):
        col_type = current_structure[col]['type']
        print(f"   {i:2d}. {col:<30} ({col_type})")
    
    print("\n‚úÖ –ö–û–õ–û–ù–ö–ò –ö–û–¢–û–†–´–ï –û–°–¢–ê–ù–£–¢–°–Ø:")
    for i, (col, col_type) in enumerate(required_columns.items(), 1):
        print(f"   {i:2d}. {col:<30} ({col_type})")
    
    print("\n" + "="*80)
    print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º")
    print("="*80)
    
    while True:
        response = input("\nü§î –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ—á–∏—Å—Ç–∫—É? (y/N/q –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").lower().strip()
        if response == 'q':
            print("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return False
        elif response == 'y':
            return True
        else:
            print("‚ùì –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ 'y' –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –∏–ª–∏ 'q' –¥–ª—è –≤—ã—Ö–æ–¥–∞")

def create_backup(table_name):
    """–°–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é —Ç–∞–±–ª–∏—Ü—ã"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_table = f"{table_name}_backup_{timestamp}"
    
    conn = get_database_connection()
    try:
        with conn.cursor() as cur:
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
            cur.execute(f"CREATE TABLE {backup_table} AS SELECT * FROM {table_name}")
            conn.commit()
            logger.success(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞: {backup_table}")
            return backup_table
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")
        raise
    finally:
        conn.close()

def cleanup_table_structure(table_name):
    """–û—á–∏—Å—Ç–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã"""
    conn = get_database_connection()
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        current_structure = get_table_structure(table_name)
        required_columns = get_required_columns()
        
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        columns_to_drop = []
        for col in current_structure.keys():
            if col not in required_columns:
                columns_to_drop.append(col)
        
        if not columns_to_drop:
            logger.info("‚úÖ –í—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —É–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ, –Ω–∏—á–µ–≥–æ —É–¥–∞–ª—è—Ç—å –Ω–µ –Ω—É–∂–Ω–æ")
            return None
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–ª–∞–Ω –∏ —Å–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        if not confirm_cleanup(columns_to_drop, current_structure):
            return None
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
        backup_table = create_backup(table_name)
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        with conn.cursor() as cur:
            for col in columns_to_drop:
                # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if col.startswith('[') and col.endswith(']'):
                    col_name = f'"{col}"'
                else:
                    col_name = col
                
                sql = f"ALTER TABLE {table_name} DROP COLUMN {col_name} CASCADE;"
                logger.info(f"üóëÔ∏è  –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É: {col}")
                cur.execute(sql)
            
            conn.commit()
        
        logger.success(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {len(columns_to_drop)} –ª–∏—à–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫")
        logger.info(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ç–∞–±–ª–∏—Ü–µ: {backup_table}")
        return backup_table
        
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")
        raise
    finally:
        conn.close()

def verify_table_structure(table_name):
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏"""
    structure = get_table_structure(table_name)
    required_columns = get_required_columns()
    
    print("\n" + "="*60)
    print("üîç –ü–†–û–í–ï–†–ö–ê –°–¢–†–£–ö–¢–£–†–´ –¢–ê–ë–õ–ò–¶–´ –ü–û–°–õ–ï –û–ß–ò–°–¢–ö–ò")
    print("="*60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    missing_columns = []
    for col, col_type in required_columns.items():
        if col in structure:
            print(f"  ‚úÖ {col:<25} ({structure[col]['type']})")
        else:
            print(f"  ‚ùå {col:<25} –û–¢–°–£–¢–°–¢–í–£–ï–¢")
            missing_columns.append(col)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
    extra_columns = []
    for col in structure.keys():
        if col not in required_columns:
            print(f"  ‚ö†Ô∏è  {col:<25} –õ–ò–®–ù–Ø–Ø –ö–û–õ–û–ù–ö–ê")
            extra_columns.append(col)
    
    print("="*60)
    
    if missing_columns:
        logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        return False
    
    if extra_columns:
        logger.warning(f"‚ö†Ô∏è  –õ–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏: {extra_columns}")
        return False
    
    logger.success("‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞!")
    return True

def find_dag_file():
    """–ù–∞–π—Ç–∏ —Ñ–∞–π–ª DAG"""
    dag_paths = [
        PROJECT_ROOT / 'company_products_etl.py',
        PROJECT_ROOT / 'dags' / 'company_products_etl.py',
        DOCKER_ROOT / 'dags' / 'oneC_etl' / 'company_products_etl.py'
    ]
    
    for dag_path in dag_paths:
        if dag_path.exists():
            return dag_path
    
    raise FileNotFoundError("–§–∞–π–ª company_products_etl.py –Ω–µ –Ω–∞–π–¥–µ–Ω")

def find_mappings_file():
    """–ù–∞–π—Ç–∏ —Ñ–∞–π–ª dax_mappings.py"""
    mappings_paths = [
        PROJECT_ROOT / 'config' / 'dax_mappings.py',
        PROJECT_ROOT / 'dax_mappings.py',
        DOCKER_ROOT / 'dags' / 'oneC_etl' / 'config' / 'dax_mappings.py'
    ]
    
    for mappings_path in mappings_paths:
        if mappings_path.exists():
            return mappings_path
    
    raise FileNotFoundError("–§–∞–π–ª dax_mappings.py –Ω–µ –Ω–∞–π–¥–µ–Ω")

def update_dag_column_mapping():
    """–û–±–Ω–æ–≤–∏—Ç—å –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –≤ DAG"""
    dag_file = find_dag_file()
    
    # –ù–æ–≤–æ–µ –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏ –∫–æ–ª–æ–Ω–æ–∫
    new_mapping = {
        "CompanyProducts[ID]": "id",
        "CompanyProducts[Description]": "description", 
        "CompanyProducts[Brand]": "brand",
        "CompanyProducts[Category]": "category",
        "CompanyProducts[Withdrawn_from_range]": "withdrawn_from_range",
        "CompanyProducts[item_number]": "item_number",
        "–£–¢_–¢–æ–≤–∞—Ä–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏[_description]": "product_category",
        "–£–¢_–†–°–≤–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ–°–≤–µ–¥–µ–Ω–∏—è2_0[–ü–æ–¥ –∑–∞–∫–∞–∑]": "on_order",
        "–í—ã–≤–æ–¥–∏—Ç—Å—è_–±–µ–∑_–æ—Å—Ç–∞—Ç–∫–æ–≤": "is_vector",
        "CountRows–£–¢_–†–°–≤–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ–°–≤–µ–¥–µ–Ω–∏—è2_0": "count_rows",
        "Product Properties": "merged"  # ‚úÖ –ú–∞–ø–ø–∏–º –≤ merged –≤–º–µ—Å—Ç–æ product_properties
    }
    
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª DAG
    with open(dag_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # –ù–∞—Ö–æ–¥–∏–º –∏ –∑–∞–º–µ–Ω—è–µ–º –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
    import re
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–æ–∫–∞ columns
    pattern = r'("columns":\s*\{[^}]*\})'
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è columns
    new_columns_content = '"columns": {\n'
    for i, (key, value) in enumerate(new_mapping.items()):
        new_columns_content += f'                "{key}": "{value}"'
        if i < len(new_mapping) - 1:
            new_columns_content += ','
        new_columns_content += '\n'
    new_columns_content += '            }'
    
    # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π –±–ª–æ–∫ –Ω–∞ –Ω–æ–≤—ã–π
    new_content = re.sub(pattern, new_columns_content, content, flags=re.DOTALL)
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    with open(dag_file, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    logger.success(f"‚úÖ –ú–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–æ –≤ {dag_file}")

def update_dax_mappings():
    """–û–±–Ω–æ–≤–∏—Ç—å –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª–µ dax_mappings.py"""
    mappings_file = find_mappings_file()
    
    # –ù–æ–≤–æ–µ –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ
    new_mapping = {
        'CompanyProducts[ID]': 'id',
        'CompanyProducts[Description]': 'description',
        'CompanyProducts[Brand]': 'brand', 
        'CompanyProducts[Category]': 'category',
        'CompanyProducts[Withdrawn_from_range]': 'withdrawn_from_range',
        'CompanyProducts[item_number]': 'item_number',
        '–£–¢_–¢–æ–≤–∞—Ä–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏[_description]': 'product_category',
        '–£–¢_–†–°–≤–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ–°–≤–µ–¥–µ–Ω–∏—è2_0[–ü–æ–¥ –∑–∞–∫–∞–∑]': 'on_order',
        '–í—ã–≤–æ–¥–∏—Ç—Å—è_–±–µ–∑_–æ—Å—Ç–∞—Ç–∫–æ–≤': 'is_vector',
        'CountRows–£–¢_–†–°–≤–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ–°–≤–µ–¥–µ–Ω–∏—è2_0': 'count_rows',
        'Product Properties': 'merged'  # ‚úÖ –ú–∞–ø–ø–∏–º –≤ merged
    }
    
    # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
    with open(mappings_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # –ù–∞—Ö–æ–¥–∏–º –∏ –∑–∞–º–µ–Ω—è–µ–º –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ DAX_MAPPINGS
    import re
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–æ–∫–∞ columns –≤ DAX_MAPPINGS
    pattern = r"'columns':\s*\{[^}]*\}"
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
    new_columns_content = "'columns': {\n"
    for i, (key, value) in enumerate(new_mapping.items()):
        new_columns_content += f"            '{key}': '{value}'"
        if i < len(new_mapping) - 1:
            new_columns_content += ','
        new_columns_content += '\n'
    new_columns_content += '        }'
    
    # –ó–∞–º–µ–Ω—è–µ–º
    new_content = re.sub(pattern, new_columns_content, content, flags=re.DOTALL)
    
    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    with open(mappings_file, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    logger.success(f"‚úÖ –ú–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–æ –≤ {mappings_file}")

def restore_from_backup(backup_table, target_table):
    """–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
    if not backup_table:
        logger.error("‚ùå –ù–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è")
        return False
    
    print(f"\nüîÑ –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {backup_table}")
    
    conn = get_database_connection()
    try:
        with conn.cursor() as cur:
            # –£–¥–∞–ª—è–µ–º —Ç–µ–∫—É—â—É—é —Ç–∞–±–ª–∏—Ü—É
            cur.execute(f"DROP TABLE IF EXISTS {target_table} CASCADE")
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
            cur.execute(f"CREATE TABLE {target_table} AS SELECT * FROM {backup_table}")
            
            conn.commit()
            logger.success(f"‚úÖ –¢–∞–±–ª–∏—Ü–∞ {target_table} –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏–∑ {backup_table}")
            return True
            
    except Exception as e:
        conn.rollback()
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
        return False
    finally:
        conn.close()

def create_cleanup_migration():
    """–°–æ–∑–¥–∞—Ç—å SQL –º–∏–≥—Ä–∞—Ü–∏—é –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–∞–±–ª–∏—Ü—ã"""
    migration_sql = f"""
-- –ú–∏–≥—Ä–∞—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü—ã companyproducts
-- –î–∞—Ç–∞: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

-- –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
CREATE TABLE companyproducts_backup_$(date +%Y%m%d_%H%M%S) AS SELECT * FROM companyproducts;

-- –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "companyproducts" CASCADE;
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "isgrandtotalrowtotal" CASCADE;
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "companyproducts_1" CASCADE;
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "companyproducts_2" CASCADE;
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "companyproducts_3" CASCADE;
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "companyproducts_4" CASCADE;
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "companyproducts_5" CASCADE;
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "[–≤—ã–≤–æ–¥–∏—Ç—Å—è_–±–µ–∑_–æ—Å—Ç–∞—Ç–∫–æ–≤]" CASCADE;
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "companyproducts[merged]" CASCADE;
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "[product properties]" CASCADE;
ALTER TABLE companyproducts DROP COLUMN IF EXISTS "product_properties" CASCADE;

-- –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
SELECT column_name, data_type, is_nullable
FROM information_schema.columns 
WHERE table_schema = 'public' 
AND table_name = 'companyproducts'
ORDER BY ordinal_position;
"""
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É migrations –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
    migrations_dir = SCRIPT_DIR.parent / 'migrations'
    migrations_dir.mkdir(exist_ok=True)
    
    migration_file = migrations_dir / 'cleanup_companyproducts_structure.sql'
    with open(migration_file, 'w', encoding='utf-8') as f:
        f.write(migration_sql)
    
    logger.success(f"üìÑ SQL –º–∏–≥—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: {migration_file}")
    return migration_file

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("\n" + "üöÄ" + "="*60 + "üöÄ")
    print("üéØ –û–ß–ò–°–¢–ö–ê –°–¢–†–£–ö–¢–£–†–´ –¢–ê–ë–õ–ò–¶–´ COMPANYPRODUCTS")
    print("üöÄ" + "="*60 + "üöÄ")
    
    try:
        table_name = 'companyproducts'
        
        # 1. –°–æ–∑–¥–∞–µ–º SQL –º–∏–≥—Ä–∞—Ü–∏—é
        logger.info("1. üìÑ –°–æ–∑–¥–∞–µ–º SQL –º–∏–≥—Ä–∞—Ü–∏—é...")
        migration_file = create_cleanup_migration()
        
        # 2. –û—á–∏—â–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã
        logger.info("2. üóëÔ∏è  –û—á–∏—â–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã...")
        backup_table = cleanup_table_structure(table_name)
        
        if backup_table is None:
            logger.info("‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞")
            return True
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        logger.info("3. üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã...")
        success = verify_table_structure(table_name)
        
        if not success:
            logger.error("‚ùå –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–∞–º–∏!")
            
            # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
            response = input("\nüîÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏? (y/N): ").lower().strip()
            if response == 'y':
                restore_from_backup(backup_table, table_name)
            
            return False
        
        # 4. –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ DAG
        logger.info("4. üîÑ –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ DAG...")
        update_dag_column_mapping()
        
        # 5. –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ dax_mappings.py
        logger.info("5. üîÑ –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ dax_mappings.py...")
        update_dax_mappings()
        
        print("\n" + "üéâ" + "="*60 + "üéâ")
        print("‚úÖ –û–ß–ò–°–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print("üéâ" + "="*60 + "üéâ")
        print(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_table}")
        print(f"üìÑ SQL –º–∏–≥—Ä–∞—Ü–∏—è: {migration_file}")
        print("üîÑ –ú–∞–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–æ –≤ DAG –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 
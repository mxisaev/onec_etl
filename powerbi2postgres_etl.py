#!/usr/bin/env python3
"""
PowerBI to PostgreSQL ETL DAG
ETL Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· PowerBI Ð² PostgreSQL
"""

import sys
import os
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable

# ðŸŽ¯ ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿ÑƒÑ‚ÐµÐ¹ Ð´Ð»Ñ utils.logger
dags_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if dags_root not in sys.path:
    sys.path.insert(0, dags_root)

# ðŸŽ¯ Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ utils.logger
from utils.logger import logger

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'PowerBI2PostgresETL',
    default_args=default_args,
    description='ETL Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· PowerBI Ð² PostgreSQL',
    schedule_interval='0 10 * * 1-5',  # ÐŸÐ½-ÐŸÑ‚ Ð² 10:00 UTC
    catchup=False,
    tags=['etl', 'powerbi', 'postgres', 'data_warehouse']
)

def extract_powerbi_data_task(**context):
    """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· PowerBI"""
    try:
        logger.info("ðŸ”„ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· PowerBI...")
        
        from oneC_etl.tasks.extract import extract_powerbi_data
        
        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡Ð¸
        task_config = {
            'dataset_id': 'afb5ea40-5805-4b0b-a082-81ca7333be85',
            'dax_query': 'company_products',  # Ð‘ÑƒÐ´ÐµÑ‚ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ Ð¸Ð· Airflow Variables
            'columns': {
                'CompanyProducts[ID]': 'id',
                'CompanyProducts[Description]': 'description',
                'CompanyProducts[Brand]': 'brand',
                'CompanyProducts[Category]': 'category',
                'CompanyProducts[Withdrawn_from_range]': 'withdrawn_from_range',
                'CompanyProducts[item_number]': 'item_number'
            }
        }
        
        result = extract_powerbi_data(task_config)
        
        logger.info(f"âœ… Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾: {len(result)} ÑÑ‚Ñ€Ð¾Ðº")
        return result
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {str(e)}")
        raise

def load_to_postgres_task(**context):
    """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² PostgreSQL"""
    try:
        logger.info("ðŸ”„ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² PostgreSQL...")
        
        from oneC_etl.tasks.load import execute_etl_task
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸
        ti = context['ti']
        data = ti.xcom_pull(task_ids='extract_powerbi_data')
        
        if data is None:
            raise ValueError("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸")
        
        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸
        task_config = {
            'source_table': 'powerbi_data',
            'target_table': 'postgres_data',
            'mapping_name': 'company_products'
        }
        
        result = execute_etl_task(data, task_config)
        
        logger.info(f"âœ… Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°: {result}")
        return result
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {str(e)}")
        raise

def validate_data_task(**context):
    """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
    try:
        logger.info("ðŸ”„ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
        
        from oneC_etl.database.client import DatabaseClient
        
        db_client = DatabaseClient()
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
        validation_query = """
        SELECT 
            COUNT(*) as total_records,
            COUNT(DISTINCT id) as unique_ids,
            COUNT(CASE WHEN description IS NOT NULL THEN 1 END) as records_with_description
        FROM postgres_data
        """
        
        result = db_client.execute_query(validation_query)
        
        if result and len(result) > 0:
            stats = result[0]
            logger.info(f"ðŸ“Š Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸: {stats}")
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ñ…
            if stats['total_records'] > 0 and stats['unique_ids'] == stats['total_records']:
                logger.info("âœ… Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾ÑˆÐ»Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾")
                return {"status": "success", "stats": stats}
            else:
                logger.warning("âš ï¸ ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ñ Ð´Ð°Ð½Ð½Ñ‹Ð¼Ð¸")
                return {"status": "warning", "stats": stats}
        else:
            logger.error("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸")
            return {"status": "error", "message": "Validation failed"}
            
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸: {str(e)}")
        raise

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡
extract_operator = PythonOperator(
    task_id='extract_powerbi_data',
    python_callable=extract_powerbi_data_task,
    dag=dag
)

load_operator = PythonOperator(
    task_id='load_to_postgres',
    python_callable=load_to_postgres_task,
    dag=dag
)

validate_operator = PythonOperator(
    task_id='validate_data',
    python_callable=validate_data_task,
    dag=dag
)

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹
extract_operator >> load_operator >> validate_operator

if __name__ == "__main__":
    dag.cli()

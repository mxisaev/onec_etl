#!/usr/bin/env python3
"""
Company Products ETL DAG
ETL Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ñ… ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ Ð¸Ð· Power BI Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð² PostgreSQL
"""

import sys
import os
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable

# ðŸŽ¯ ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿ÑƒÑ‚ÐµÐ¹ Ð´Ð»Ñ utils.logger
dags_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if dags_root not in sys.path:
    sys.path.insert(0, dags_root)

# ðŸŽ¯ Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ utils.logger
from utils.logger import get_logger

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ logger Ð´Ð»Ñ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ DAG'Ð°
logger = get_logger("company_products_etl", "oneC_etl")

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'CompanyProductsETL',
    default_args=default_args,
    description='ETL Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ñ… ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ Ð¸Ð· Power BI Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð² PostgreSQL',
    schedule_interval='35 4-13 * * 1-5',  # ÐŸÐ½-ÐŸÑ‚ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 35 Ð¼Ð¸Ð½ÑƒÑ‚ Ñ 4:00 Ð´Ð¾ 13:00 UTC (9:00-18:00 UTC+5)
    catchup=False,
    tags=['etl', 'powerbi', 'postgres', 'company_products', 'product_properties']
)

def extract_powerbi_data_task(**context):
    """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ñ… Ð¸Ð· Power BI Ñ‡ÐµÑ€ÐµÐ· DAX"""
    try:
        logger.info("ðŸ”„ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ñ… Ð¸Ð· Power BI...")
        
        from oneC_etl.tasks.extract import extract_powerbi_data
        
        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡Ð¸ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸
        task_config = {
            'dataset_id': '022e7796-b30f-44d4-b076-15331e612d47',  # 1cExportDataset (Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚)
            'dax_query': 'company_products',  # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ÑÑ Ð¸Ð· Airflow Variables
            'columns': {
                'CompanyProducts[ID]': 'id',
                'CompanyProducts[Description]': 'description',
                'CompanyProducts[Brand]': 'brand',
                'CompanyProducts[Category]': 'category',
                'CompanyProducts[Withdrawn_from_range]': 'withdrawn_from_range',
                'CompanyProducts[item_number]': 'item_number',
                '[Product_Properties]': 'product_properties',
                'Ð£Ð¢_Ð¢Ð¾Ð²Ð°Ñ€Ð½Ñ‹Ðµ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸[_description]': 'product_category',
                'Ð£Ð¢_Ð Ð¡Ð²Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹ÐµÐ¡Ð²ÐµÐ´ÐµÐ½Ð¸Ñ2_0[ÐŸÐ¾Ð´ Ð·Ð°ÐºÐ°Ð·]': 'on_order',
                'Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ñ‚ÑÑ_Ð±ÐµÐ·_Ð¾ÑÑ‚Ð°Ñ‚ÐºÐ¾Ð²': 'is_vector',
                'CountRowsÐ£Ð¢_Ð Ð¡Ð²Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹ÐµÐ¡Ð²ÐµÐ´ÐµÐ½Ð¸Ñ2_0': 'count_rows'
            }
        }
        
        result = extract_powerbi_data(task_config)
        return result
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {str(e)}")
        raise

def load_to_postgres_task(**context):
    """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ñ… Ð² PostgreSQL"""
    try:
        logger.info("ðŸ”„ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² PostgreSQL...")
        
        from oneC_etl.tasks.load import execute_etl_task
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸
        ti = context['ti']
        data = ti.xcom_pull(task_ids='extract_powerbi_data')
        
        if data is None:
            raise ValueError("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸")
        
        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·ÑƒÐµÐ¼ ÑÐ¿Ð¸ÑÐ¾Ðº ÑÐ»Ð¾Ð²Ð°Ñ€ÐµÐ¹ Ð² pandas DataFrame
        import pandas as pd
        df = pd.DataFrame(data)
        
        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸
        task_config = {
            'source_table': 'powerbi_company_products',
            'target_table': 'companyproducts',
            'mapping_name': 'company_products'
        }
        
        result = execute_etl_task(df, task_config)
        return result
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…: {str(e)}")
        raise



def validate_data_task(**context):
    """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ñ…"""
    try:
        logger.info("ðŸ”„ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸ÑŽ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ñ…...")
        
        from oneC_etl.services.postgres.client import PostgresClient
        
        db_client = PostgresClient()
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð²
        products_query = """
        SELECT 
            COUNT(*) as total_products,
            COUNT(CASE WHEN description IS NOT NULL THEN 1 END) as products_with_description,
            COUNT(CASE WHEN brand IS NOT NULL THEN 1 END) as products_with_brand,
            COUNT(CASE WHEN category IS NOT NULL THEN 1 END) as products_with_category,
            COUNT(CASE WHEN item_number IS NOT NULL THEN 1 END) as products_with_item_number
        FROM companyproducts
        """
        
        products_result = db_client.execute_query(products_query)
        
        if products_result:
            products_stats = products_result[0]
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ñ…
            validation_status = "success"
            if products_stats['total_products'] == 0:
                validation_status = "error"
                logger.error("âŒ Ð’ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ Ð½ÐµÑ‚ Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð²")
            elif products_stats['products_with_description'] == 0:
                validation_status = "warning"
                logger.warning("âš ï¸ Ð£ Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð² Ð½ÐµÑ‚ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ð¹")
            elif products_stats['products_with_brand'] == 0:
                logger.warning("âš ï¸ Ð£ Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð² Ð½ÐµÑ‚ Ð±Ñ€ÐµÐ½Ð´Ð¾Ð²")
            else:
                logger.info("âœ… Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ñ… Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾")
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ Ð¸Ð· Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸
            ti = context['ti']
            cleanup_result = ti.xcom_pull(task_ids='cleanup_orphaned_records')
            
            if cleanup_result and cleanup_result.get('status') == 'success':
                logger.info(f"ðŸ—‘ï¸ ÐžÑ‡Ð¸ÑÑ‚ÐºÐ°: ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ {cleanup_result.get('deleted_records', 0)} ÑƒÑÑ‚Ð°Ñ€ÐµÐ²ÑˆÐ¸Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
            
            result = {
                "status": validation_status,
                "products": products_stats,
                "cleanup": cleanup_result,
                "message": f"ÐŸÑ€Ð¾Ð²ÐµÑ€ÐµÐ½Ð¾ {products_stats['total_products']} Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð²"
            }
            
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ñ‚Ð¾Ð³Ð¾Ð²ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ
            logger.info("ðŸŽ¯ ========================================")
            logger.info(f"ðŸ“Š Ð˜Ñ‚Ð¾Ð³Ð¾Ð²Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°:")
            logger.info(f"   - Ð’ÑÐµÐ³Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð¾Ð²: {products_stats['total_products']}")
            if cleanup_result and cleanup_result.get('status') == 'success':
                logger.info(f"   - Ð£Ð´Ð°Ð»ÐµÐ½Ð¾ ÑƒÑÑ‚Ð°Ñ€ÐµÐ²ÑˆÐ¸Ñ…: {cleanup_result.get('deleted_records', 0)}")
            logger.info("ðŸŽ¯ ========================================")
            
            return result
        else:
            logger.warning("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸")
            return {"status": "error", "message": "Validation failed"}
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸: {str(e)}")
        raise

def cleanup_orphaned_records_task(**context):
    """Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… Ð½ÐµÑ‚ Ð² Ð½Ð¾Ð²Ð¾Ð¹ Ð²Ñ‹Ð³Ñ€ÑƒÐ·ÐºÐµ PowerBI"""
    try:
        logger.info("ðŸ”„ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÑƒ ÑƒÑÑ‚Ð°Ñ€ÐµÐ²ÑˆÐ¸Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹...")
        
        from oneC_etl.tasks.cleanup import cleanup_orphaned_records
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸
        ti = context['ti']
        data = ti.xcom_pull(task_ids='extract_powerbi_data')
        
        if data is None:
            raise ValueError("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸")
        
        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸
        cleanup_config = {
            'source_table': 'powerbi_company_products',
            'target_table': 'companyproducts',
            'key_column': 'id'  # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ 'id' ÐºÐ°Ðº Ð² PowerBI Ð´Ð°Ð½Ð½Ñ‹Ñ…
        }
        
        result = cleanup_orphaned_records(data, cleanup_config)
        return result
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸ ÑƒÑÑ‚Ð°Ñ€ÐµÐ²ÑˆÐ¸Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹: {str(e)}")
        raise

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ðµ Ð¸Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸
extract_operator = PythonOperator(
    task_id='extract_powerbi_data',
    python_callable=extract_powerbi_data_task,
    dag=dag
)

load_operator = PythonOperator(
    task_id='load_to_postgres',
    python_callable=load_to_postgres_task,
    dag=dag
)



validate_operator = PythonOperator(
    task_id='validate_data',
    python_callable=validate_data_task,
    dag=dag
)

cleanup_operator = PythonOperator(
    task_id='cleanup_orphaned_records',
    python_callable=cleanup_orphaned_records_task,
    dag=dag
)

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð¿Ð¾Ñ‚Ð¾ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ…:
# 1. Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Power BI Ñ‡ÐµÑ€ÐµÐ· DAX
# 2. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð² Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ companyproducts
# 3. Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ñ‚Ð¾Ð²Ð°Ñ€Ð°Ñ…
# 4. ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° ÑƒÑÑ‚Ð°Ñ€ÐµÐ²ÑˆÐ¸Ñ… Ð·Ð°Ð¿Ð¸ÑÐµÐ¹
extract_operator >> load_operator >> validate_operator >> cleanup_operator

if __name__ == "__main__":
    dag.cli()

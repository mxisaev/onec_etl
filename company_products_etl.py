#!/usr/bin/env python3
"""
Company Products ETL DAG
ETL –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ Power BI –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ PostgreSQL
"""

import sys
import os
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable

# üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π –¥–ª—è utils.logger
dags_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if dags_root not in sys.path:
    sys.path.insert(0, dags_root)

# üéØ –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º utils.logger
from utils.logger import get_logger

# –°–æ–∑–¥–∞–µ–º logger –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ DAG'–∞
logger = get_logger("company_products_etl", "oneC_etl")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'CompanyProductsETL',
    default_args=default_args,
    description='ETL –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ Power BI –∏ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ PostgreSQL',
    schedule_interval='35 4-13 * * 1-5',  # –ü–Ω-–ü—Ç –∫–∞–∂–¥—ã–µ 35 –º–∏–Ω—É—Ç —Å 4:00 –¥–æ 13:00 UTC (9:00-18:00 UTC+5)
    catchup=False,
    tags=['etl', 'powerbi', 'postgres', 'company_products', 'product_properties']
)

def extract_powerbi_data_task(**context):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö –∏–∑ Power BI —á–µ—Ä–µ–∑ DAX"""
    try:
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö –∏–∑ Power BI...")
        
        from oneC_etl.tasks.extract import extract_powerbi_data
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        task_config = {
            'dataset_id': '022e7796-b30f-44d4-b076-15331e612d47',  # 1cExportDataset (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç)
            'dax_query': 'company_products',  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ Airflow Variables
            'columns': {
                'CompanyProducts[ID]': 'id',
                'CompanyProducts[Description]': 'description',
                'CompanyProducts[Brand]': 'brand',
                'CompanyProducts[Category]': 'category',
                'CompanyProducts[Withdrawn_from_range]': 'withdrawn_from_range',
                'CompanyProducts[item_number]': 'item_number',
                '[Product_Properties]': 'product_properties',
                '–£–¢_–¢–æ–≤–∞—Ä–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏[_description]': 'product_category',
                '–£–¢_–†–°–≤–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ–°–≤–µ–¥–µ–Ω–∏—è2_0[–ü–æ–¥ –∑–∞–∫–∞–∑]': 'on_order',
                '–í—ã–≤–æ–¥–∏—Ç—Å—è_–±–µ–∑_–æ—Å—Ç–∞—Ç–∫–æ–≤': 'is_vector',
                'CountRows–£–¢_–†–°–≤–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ–°–≤–µ–¥–µ–Ω–∏—è2_0': 'count_rows'
            }
        }
        
        result = extract_powerbi_data(task_config)
        return result
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        raise

def load_to_postgres_task(**context):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö –≤ PostgreSQL"""
    try:
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL...")
        
        from oneC_etl.tasks.load import execute_etl_task
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏
        ti = context['ti']
        data = ti.xcom_pull(task_ids='extract_powerbi_data')
        
        if data is None:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π –≤ pandas DataFrame
        import pandas as pd
        df = pd.DataFrame(data)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        task_config = {
            'source_table': 'powerbi_company_products',
            'target_table': 'companyproducts',
            'mapping_name': 'company_products'
        }
        
        result = execute_etl_task(df, task_config)
        return result
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        raise



def validate_data_task(**context):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö"""
    try:
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö...")
        
        from oneC_etl.services.postgres.client import PostgresClient
        
        db_client = PostgresClient()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤
        products_query = """
        SELECT 
            COUNT(*) as total_products,
            COUNT(CASE WHEN description IS NOT NULL THEN 1 END) as products_with_description,
            COUNT(CASE WHEN brand IS NOT NULL THEN 1 END) as products_with_brand,
            COUNT(CASE WHEN category IS NOT NULL THEN 1 END) as products_with_category,
            COUNT(CASE WHEN item_number IS NOT NULL THEN 1 END) as products_with_item_number
        FROM companyproducts
        """
        
        products_result = db_client.execute_query(products_query)
        
        if products_result:
            products_stats = products_result[0]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            validation_status = "success"
            if products_stats['total_products'] == 0:
                validation_status = "error"
                logger.error("‚ùå –í —Ç–∞–±–ª–∏—Ü–µ –Ω–µ—Ç —Ç–æ–≤–∞—Ä–æ–≤")
            elif products_stats['products_with_description'] == 0:
                validation_status = "warning"
                logger.warning("‚ö†Ô∏è –£ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–π")
            elif products_stats['products_with_brand'] == 0:
                logger.warning("‚ö†Ô∏è –£ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ—Ç –±—Ä–µ–Ω–¥–æ–≤")
            else:
                logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–∞—Ö –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—á–∏—Å—Ç–∫–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏
            ti = context['ti']
            cleanup_result = ti.xcom_pull(task_ids='cleanup_orphaned_records')
            
            if cleanup_result and cleanup_result.get('status') == 'success':
                logger.info(f"üóëÔ∏è –û—á–∏—Å—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–æ {cleanup_result.get('deleted_records', 0)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π")
            
            result = {
                "status": validation_status,
                "products": products_stats,
                "cleanup": cleanup_result,
                "message": f"–ü—Ä–æ–≤–µ—Ä–µ–Ω–æ {products_stats['total_products']} —Ç–æ–≤–∞—Ä–æ–≤"
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
            logger.info("üéØ ========================================")
            logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            logger.info(f"   - –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {products_stats['total_products']}")
            if cleanup_result and cleanup_result.get('status') == 'success':
                logger.info(f"   - –£–¥–∞–ª–µ–Ω–æ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö: {cleanup_result.get('deleted_records', 0)}")
            logger.info("üéØ ========================================")
            
            return result
        else:
            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
            return {"status": "error", "message": "Validation failed"}
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}")
        raise

def cleanup_orphaned_records_task(**context):
    """–£–¥–∞–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –Ω–æ–≤–æ–π –≤—ã–≥—Ä—É–∑–∫–µ PowerBI"""
    try:
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ—á–∏—Å—Ç–∫—É —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π...")
        
        from oneC_etl.tasks.cleanup import cleanup_orphaned_records
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏
        ti = context['ti']
        data = ti.xcom_pull(task_ids='extract_powerbi_data')
        
        if data is None:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—á–∏—Å—Ç–∫–∏")
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏
        cleanup_config = {
            'source_table': 'powerbi_company_products',
            'target_table': 'companyproducts',
            'key_column': 'id'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 'id' –∫–∞–∫ –≤ PowerBI –¥–∞–Ω–Ω—ã—Ö
        }
        
        result = cleanup_orphaned_records(data, cleanup_config)
        return result
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π: {str(e)}")
        raise

# –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á —Å–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
extract_operator = PythonOperator(
    task_id='extract_powerbi_data',
    python_callable=extract_powerbi_data_task,
    dag=dag
)

load_operator = PythonOperator(
    task_id='load_to_postgres',
    python_callable=load_to_postgres_task,
    dag=dag
)



validate_operator = PythonOperator(
    task_id='validate_data',
    python_callable=validate_data_task,
    dag=dag
)

cleanup_operator = PythonOperator(
    task_id='cleanup_orphaned_records',
    python_callable=cleanup_orphaned_records_task,
    dag=dag
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–æ–≥–ª–∞—Å–Ω–æ –ø–æ—Ç–æ–∫—É –¥–∞–Ω–Ω—ã—Ö:
# 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Power BI —á–µ—Ä–µ–∑ DAX
# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü—É companyproducts
# 3. –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ç–æ–≤–∞—Ä–∞—Ö
# 4. –û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π
extract_operator >> load_operator >> validate_operator >> cleanup_operator

if __name__ == "__main__":
    dag.cli()

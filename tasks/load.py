"""
PostgreSQL data loading module
"""

import pandas as pd
from loguru import logger
from oneC_etl.services.postgres.client import PostgresClient
from oneC_etl.config.settings import get_config
from oneC_etl.config.dax_mappings import get_dax_mapping
from oneC_etl.utils.dax_utils import get_business_columns_from_dax

def execute_etl_task(data, task):
    """
    Execute ETL task - load data to PostgreSQL
    
    Args:
        data (pd.DataFrame): Data to load
        task (dict): Task configuration containing:
            - target_table: Target table name
            - source_table: Source table name
            - mapping_name: Name of the DAX mapping to use
    
    Returns:
        dict: Operation statistics
    """
    try:
        logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü—É {task['target_table']}")
        
        # Get configuration
        config = get_config()
        
        # Get DAX mapping
        mapping = get_dax_mapping(task['mapping_name'])
        
        # Initialize PostgreSQL client
        client = PostgresClient()
        
        # Prepare data for loading
        data['is_vector'] = False  # Mark records for vector update
        
        # Rename columns according to mapping
        data = data.rename(columns=mapping['columns'])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∞–±–ª–∏—Ü—ã
        if task['target_table'] == 'partners':
            # –î–ª—è partners –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –∫–ª—é—á –∏–∑ 1–°
            key_columns = ['id_1c_partner', 'id_1c_contact']
            if not all(col in data.columns for col in key_columns):
                raise ValueError(f"Required columns {key_columns} not found in data for table {task['target_table']}")
        else:
            # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∞–±–ª–∏—Ü –∏—Å–ø–æ–ª—å–∑—É–µ–º 'id'
            key_columns = ['id']
            if 'id' not in data.columns:
                raise ValueError(f"Required column 'id' not found in data for table {task['target_table']}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π (–∏—Å–∫–ª—é—á–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è)
        if task['target_table'] == 'partners':
            technical_fields = {'partner_uid', 'id_1c_partner', 'id_1c_contact', 'is_vector', 'created_at', 'updated_at', 'vector', 'extracted_at'}
        else:
            technical_fields = {'id', 'item_number', 'is_vector', 'upload_timestamp', 'updated_at', 'vector', 'extracted_at'}
        
        columns_for_change_analysis = [col for col in data.columns if col not in technical_fields]

        # Load data in batches
        batch_size = config['batch_size']
        total_rows = len(data)
        processed_rows = 0
        updated_rows = 0
        
        for i in range(0, total_rows, batch_size):
            batch = data.iloc[i:i + batch_size]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫
            def get_column_type(col_name):
                if col_name == 'id':
                    return 'UUID'
                elif col_name == 'partner_uid':
                    return 'UUID'
                elif col_name in ['id_1c_partner', 'id_1c_contact']:
                    return 'VARCHAR'
                elif col_name in ['withdrawn_from_range', 'on_order', 'is_vector', 'is_client', 'is_supplier']:
                    return 'BOOLEAN'
                else:
                    return 'TEXT'
            
            # Merge data (upsert)
            result = client.merge_data(
                table_name=task['target_table'],
                data=batch,
                key_columns=key_columns,
                columns=[{
                    'name': col,
                    'dataType': get_column_type(col)
                } for col in batch.columns],
                template_name=mapping['table_template'],
                columns_for_change_analysis=columns_for_change_analysis
            )
            
            processed_rows += len(batch)
            updated_rows += result.get('updated_rows', 0)
            
            logger.info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_rows}/{total_rows} —Å—Ç—Ä–æ–∫")
        
        stats = {
            'source': task['source_table'],
            'target': task['target_table'],
            'total_rows': total_rows,
            'processed_rows': processed_rows,
            'updated_rows': updated_rows,
            'status': 'success'
        }
        
        logger.info(f"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ —Ç–∞–±–ª–∏—Ü—É {task['target_table']}")
        return stats
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL: {str(e)}")
        return {
            'source': task['source_table'],
            'target': task['target_table'],
            'error': str(e),
            'status': 'failed'
        } 
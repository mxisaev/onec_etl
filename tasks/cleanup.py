"""
Data cleanup module for removing orphaned records
"""

import pandas as pd
from loguru import logger
from oneC_etl.services.postgres.client import PostgresClient
from oneC_etl.config.settings import get_config

def cleanup_orphaned_records(data, task_config):
    """
    Remove records from target table that are not present in the new PowerBI export
    
    Args:
        data (list): List of dictionaries from PowerBI export
        task_config (dict): Task configuration containing:
            - target_table: Target table name to clean
            - key_column: Primary key column name (usually 'id')
    
    Returns:
        dict: Cleanup statistics
    """
    try:
        # Get configuration
        config = get_config()
        
        # Initialize PostgreSQL client
        client = PostgresClient()
        
        # Convert data to DataFrame if it's a list
        if isinstance(data, list):
            df = pd.DataFrame(data)
        else:
            df = data
        
        # Get the key column values from new data
        key_column = task_config['key_column']
        target_table = task_config['target_table']
        
        if key_column not in df.columns:
            raise ValueError(f"Required key column '{key_column}' not found in data")
        
        logger.info(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ:")
        logger.info(f"   - –ö–æ–ª–æ–Ω–∫–∞ –∫–ª—é—á–∞: {key_column}")
        logger.info(f"   - –¶–µ–ª–µ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞: {target_table}")
        logger.info(f"   - –ù–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ PowerBI: {len(df)}")
        
        # Get all existing IDs from the target table
        existing_ids_query = f"""
        SELECT {key_column} 
        FROM {target_table}
        """
        
        existing_records = client.execute_query(existing_ids_query)
        existing_ids = set(record[key_column] for record in existing_records)
        
        logger.info(f"   - –°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ –ë–î: {len(existing_ids)}")
        
        # Get IDs from new PowerBI export
        new_ids = set(df[key_column].dropna().astype(str))
        
        logger.info(f"   - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ID –≤ –Ω–æ–≤–æ–π –≤—ã–≥—Ä—É–∑–∫–µ: {len(new_ids)}")
        
        # Debug: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ ID
        if existing_ids:
            sample_existing = list(existing_ids)[:3]
            logger.info(f"   - –ü—Ä–∏–º–µ—Ä—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö ID: {sample_existing}")
        
        if new_ids:
            sample_new = list(new_ids)[:3]
            logger.info(f"   - –ü—Ä–∏–º–µ—Ä—ã –Ω–æ–≤—ã—Ö ID: {sample_new}")
        
        # –ü—Ä–∏–≤–æ–¥–∏–º ID –∫ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        # –ï—Å–ª–∏ –≤ –ë–î UUID –æ–±—ä–µ–∫—Ç—ã, –∞ –∏–∑ PowerBI —Å—Ç—Ä–æ–∫–∏ - –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç—Ä–æ–∫–∞–º
        existing_ids_normalized = set(str(uid) for uid in existing_ids)
        new_ids_normalized = set(str(uid) for uid in new_ids)
        
        logger.info(f"   - –ü–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:")
        logger.info(f"     - –°—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö ID: {len(existing_ids_normalized)}")
        logger.info(f"     - –ù–æ–≤—ã—Ö ID: {len(new_ids_normalized)}")
        
        # Find orphaned records (exist in DB but not in new export)
        orphaned_ids = existing_ids_normalized - new_ids_normalized
        
        logger.info(f"   - –ù–∞–π–¥–µ–Ω–æ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π: {len(orphaned_ids)}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        if len(orphaned_ids) > len(existing_ids_normalized) * 0.9:  # –ï—Å–ª–∏ —É–¥–∞–ª—è–µ–º –±–æ–ª—å—à–µ 90% –∑–∞–ø–∏—Å–µ–π
            logger.warning(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ü–æ–ø—ã—Ç–∫–∞ —É–¥–∞–ª–∏—Ç—å {len(orphaned_ids)} –∏–∑ {len(existing_ids_normalized)} –∑–∞–ø–∏—Å–µ–π (>90%)!")
            logger.warning(f"‚ö†Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—à–∏–±–∫–æ–π. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ...")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –±–æ–ª—å—à–µ –¥–µ—Ç–∞–ª–µ–π
            if existing_ids_normalized:
                sample_existing = list(existing_ids_normalized)[:5]
                logger.warning(f"‚ö†Ô∏è –ü—Ä–∏–º–µ—Ä—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö ID (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ): {sample_existing}")
            
            if new_ids_normalized:
                sample_new = list(new_ids_normalized)[:5]
                logger.warning(f"‚ö†Ô∏è –ü—Ä–∏–º–µ—Ä—ã –Ω–æ–≤—ã—Ö ID (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ): {sample_new}")
            
            if orphaned_ids:
                sample_orphaned = list(orphaned_ids)[:5]
                logger.warning(f"‚ö†Ô∏è –ü—Ä–∏–º–µ—Ä—ã —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö ID: {sample_orphaned}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
            intersection = existing_ids_normalized & new_ids_normalized
            logger.warning(f"‚ö†Ô∏è –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ (–æ–±—â–∏–µ ID): {len(intersection)}")
            
            if len(intersection) < len(existing_ids_normalized) * 0.1:  # –ï—Å–ª–∏ –æ–±—â–∏—Ö ID –º–µ–Ω—å—à–µ 10%
                logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –û–±—â–∏—Ö ID —Ç–æ–ª—å–∫–æ {len(intersection)} –∏–∑ {len(existing_ids_normalized)}!")
                logger.error(f"‚ùå –í–æ–∑–º–æ–∂–Ω–æ, –∫–æ–ª–æ–Ω–∫–∞ –∫–ª—é—á–∞ '{key_column}' –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏!")
                logger.error(f"‚ùå –û—á–∏—Å—Ç–∫–∞ –û–¢–ú–ï–ù–ï–ù–ê –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏!")
                
                return {
                    'target_table': target_table,
                    'total_existing': len(existing_ids_normalized),
                    'total_new': len(new_ids_normalized),
                    'deleted_records': 0,
                    'status': 'error',
                    'message': f'Cleanup cancelled: only {len(intersection)} common IDs found, possible key column mismatch'
                }
        
        if orphaned_ids:
            sample_orphaned = list(orphaned_ids)[:3]
            logger.info(f"   - –ü—Ä–∏–º–µ—Ä—ã —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö ID: {sample_orphaned}")
        
        if not orphaned_ids:
            logger.info("‚úÖ –ù–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            return {
                'target_table': target_table,
                'total_existing': len(existing_ids),
                'total_new': len(new_ids),
                'deleted_records': 0,
                'status': 'success',
                'message': 'No orphaned records found'
            }
        
        # Delete orphaned records in batches
        batch_size = config.get('batch_size', 1000)
        total_orphaned = len(orphaned_ids)
        deleted_count = 0
        
        logger.info(f"üóëÔ∏è –ù–∞—á–∏–Ω–∞–µ–º —É–¥–∞–ª–µ–Ω–∏–µ {total_orphaned} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –ø–∞–∫–µ—Ç–∞–º–∏ –ø–æ {batch_size}")
        
        # Convert set to list for batching
        orphaned_ids_list = list(orphaned_ids)
        
        for i in range(0, total_orphaned, batch_size):
            batch_ids = orphaned_ids_list[i:i + batch_size]
            
            # Create placeholders for IN clause
            placeholders = ','.join(['%s'] * len(batch_ids))
            
            delete_query = f"""
            DELETE FROM {target_table}
            WHERE {key_column} IN ({placeholders})
            """
            
            logger.debug(f"üóëÔ∏è –í—ã–ø–æ–ª–Ω—è–µ–º DELETE: {delete_query}")
            logger.debug(f"üóëÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {batch_ids[:3]}... (–≤—Å–µ–≥–æ {len(batch_ids)})")
            
            # Execute delete
            with client.engine.connect() as conn:
                with conn.begin():
                    result = conn.execute(delete_query, batch_ids)
                    batch_deleted = result.rowcount
                    deleted_count += batch_deleted
                    
                    logger.info(f"üóëÔ∏è –ü–∞–∫–µ—Ç {i//batch_size + 1}: —É–¥–∞–ª–µ–Ω–æ {batch_deleted} –∑–∞–ø–∏—Å–µ–π")
                    logger.info(f"üóëÔ∏è –í—Å–µ–≥–æ —É–¥–∞–ª–µ–Ω–æ: {deleted_count}/{total_orphaned}")
        
        # Log final statistics
        logger.info(f"üéØ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —É–¥–∞–ª–µ–Ω–æ {deleted_count} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        final_check_query = f"SELECT COUNT(*) as total FROM {target_table}"
        final_result = client.execute_query(final_check_query)
        final_count = final_result[0]['total'] if final_result else 0
        
        logger.info(f"üìä –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ: {final_count} –∑–∞–ø–∏—Å–µ–π –≤ {target_table}")
        
        stats = {
            'target_table': target_table,
            'total_existing': len(existing_ids_normalized),
            'total_new': len(new_ids_normalized),
            'deleted_records': deleted_count,
            'final_count': final_count,
            'status': 'success',
            'message': f'Successfully deleted {deleted_count} orphaned records'
        }
        
        return stats
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π: {str(e)}")
        return {
            'target_table': task_config.get('target_table', 'unknown'),
            'error': str(e),
            'status': 'failed'
        }

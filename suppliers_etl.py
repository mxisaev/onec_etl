#!/usr/bin/env python3
"""
Suppliers ETL DAG
ETL –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞—Ö (–ø–æ—Å—Ç–∞–≤—â–∏–∫–∞—Ö –∏ –∫–ª–∏–µ–Ω—Ç–∞—Ö) –∏–∑ Power BI –≤ PostgreSQL
"""

import sys
import os
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable

# üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π –¥–ª—è utils.logger
dags_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if dags_root not in sys.path:
    sys.path.insert(0, dags_root)

# üéØ –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º utils.logger
from utils.logger import get_logger

# –°–æ–∑–¥–∞–µ–º logger –¥–ª—è —ç—Ç–æ–≥–æ DAG
logger = get_logger("suppliers_etl", "suppliers_etl")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'SuppliersETL',
    default_args=default_args,
    description='ETL –ø—Ä–æ—Ü–µ—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞—Ö (–ø–æ—Å—Ç–∞–≤—â–∏–∫–∞—Ö –∏ –∫–ª–∏–µ–Ω—Ç–∞—Ö) –∏–∑ Power BI –≤ PostgreSQL',
    schedule_interval='50 6,10 * * 1-5',  # –ü–Ω-–ü—Ç –≤ 6:50 –∏ 10:50 UTC (11:50 –∏ 15:50 UTC+5)
    catchup=False,
    tags=['etl', 'powerbi', 'postgres', 'suppliers', 'partners']
)

def extract_suppliers_data_task(**context):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞—Ö –∏–∑ Power BI —á–µ—Ä–µ–∑ DAX"""
    try:
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞—Ö –∏–∑ Power BI...")
        
        from suppliers_etl.tasks.extract import extract_powerbi_data
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é dataset –∏–∑ Airflow Variables
        from airflow.models import Variable
        datasets = Variable.get('datasets', deserialize_json=True)
        partners_dataset = datasets['partners']
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        task_config = {
            'dataset_id': partners_dataset['id'],  # –ü–æ–ª—É—á–∞–µ–º –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π datasets
            'dax_query_key': 'partners',  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª—é—á 'partners' –∏–∑ Airflow Variables
            'columns': {
                '–£–¢_–ü–∞—Ä—Ç–Ω–µ—Ä—ã[–ü–∞—Ä—Ç–Ω–µ—Ä.–£–¢11]': 'partner',
                '–£–¢_–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –ª–∏—Ü–∞ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤[–ö–æ–Ω—Ç–∞–∫—Ç–Ω–æ–µ –ª–∏—Ü–æ]': 'contact',
                '–£–¢_–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –ª–∏—Ü–∞ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤[email]': 'contact_email',
                '–£–¢_–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏[_description]': 'responsible_manager',
                '–£–¢_–ü–∞—Ä—Ç–Ω–µ—Ä—ã[id_1c]': 'id_1c_partner',
                '–£–¢_–ü–∞—Ä—Ç–Ω–µ—Ä—ã[is_client]': 'is_client',
                '–£–¢_–ü–∞—Ä—Ç–Ω–µ—Ä—ã[is_supplier]': 'is_supplier',
                '–£–¢_–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –ª–∏—Ü–∞ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤[–†–æ–ª—å]': 'role',
                '–£–¢_–ö–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –ª–∏—Ü–∞ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤[id_1c]': 'id_1c_contact'
            }
        }
        
        result = extract_powerbi_data(task_config)
        return result
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞—Ö: {str(e)}")
        raise

def load_partners_to_postgres_task(**context):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞—Ö –≤ PostgreSQL"""
    try:
        logger.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –≤ PostgreSQL...")
        
        # –ì–æ—Ä—è—á–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥—É–ª–µ–π, —á—Ç–æ–±—ã –≤–æ—Ä–∫–µ—Ä –ø–æ–¥—Ü–µ–ø–∏–ª –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π PostgresClient/merge
        import importlib
        import suppliers_etl.services.postgres.client as pg_client
        import suppliers_etl.tasks.load as load_module
        importlib.reload(pg_client)
        importlib.reload(load_module)
        execute_etl_task = load_module.execute_etl_task
        
        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∑–∞–¥–∞—á–∏
        ti = context['ti']
        data = ti.xcom_pull(task_ids='extract_suppliers_data')
        
        if data is None:
            raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞—Ö –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ DataFrame
        import pandas as pd
        df = pd.DataFrame(data)
        
        logger.info(f"üìä DataFrame —Å–æ–∑–¥–∞–Ω: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü –¥–ª—è PostgreSQL
        from airflow.models import Variable
        datasets = Variable.get('datasets', deserialize_json=True)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        task_config = {
            'source_table': datasets['partners']['source_table'],
            'target_table': datasets['partners']['target_table'],
            'mapping_name': 'partners'
        }
        
        # –ü—Ä–æ–≤–æ–¥–∏–º –∑–∞–≥—Ä—É–∑–∫—É –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ PostgresClient, —á—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ MERGE
        from suppliers_etl.config.dax_mappings import get_dax_mapping
        mapping = get_dax_mapping(task_config['mapping_name'])
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df['is_vector'] = False
        df = df.rename(columns=mapping['columns'])
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ –±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –±–∏–∑–Ω–µ—Å-–∫–ª—é—á–∞ id_1c_partner
        before = len(df)
        df = df.dropna(subset=['id_1c_partner'])
        df = df[df['id_1c_partner'].astype(str).str.strip() != '']
        dropped = before - len(df)
        if dropped > 0:
            logger.info(f"üßπ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π –±–µ–∑ id_1c_partner: {dropped}")
        key_columns = ['id_1c_partner', 'id_1c_contact']
        
        # –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        technical_fields = {'partner_uid', 'id_1c_partner', 'id_1c_contact', 'is_vector', 'created_at', 'updated_at', 'vector', 'extracted_at'}
        columns_for_change_analysis = [col for col in df.columns if col not in technical_fields]
        
        # –¢–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫
        def get_column_type(col_name):
            if col_name in ['partner_uid']:
                return 'UUID'
            elif col_name in ['id_1c_partner', 'id_1c_contact']:
                return 'VARCHAR'
            elif col_name in ['is_client', 'is_supplier', 'is_vector']:
                return 'BOOLEAN'
            else:
                return 'TEXT'
        
        PostgresClient = pg_client.PostgresClient
        client = PostgresClient()
        result = client.merge_data(
            table_name=task_config['target_table'],
            data=df,
            key_columns=key_columns,
            columns=[{'name': col, 'dataType': get_column_type(col)} for col in df.columns],
            template_name=mapping['table_template'],
            columns_for_change_analysis=columns_for_change_analysis
        )
        
        return result
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞—Ö: {str(e)}")
        raise

def validate_partners_data_task(**context):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞—Ö"""
    try:
        from suppliers_etl.services.postgres.client import PostgresClient
        
        db_client = PostgresClient()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤
        partners_query = """
        SELECT 
            COUNT(*) as total_partners,
            COUNT(CASE WHEN partner IS NOT NULL THEN 1 END) as partners_with_name,
            COUNT(CASE WHEN contact IS NOT NULL THEN 1 END) as partners_with_contact,
            COUNT(CASE WHEN contact_email IS NOT NULL THEN 1 END) as partners_with_email,
            COUNT(CASE WHEN responsible_manager IS NOT NULL THEN 1 END) as partners_with_manager,
            COUNT(CASE WHEN is_client = TRUE THEN 1 END) as total_clients,
            COUNT(CASE WHEN is_supplier = TRUE THEN 1 END) as total_suppliers
        FROM partners
        """
        
        result = db_client.execute_query(partners_query)
        
        if result and len(result) > 0:
            stats = result[0]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            validation_status = "success"
            if stats['total_partners'] == 0:
                validation_status = "error"
            elif stats['partners_with_name'] == 0:
                validation_status = "warning"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π –∫–ª—é—á
            composite_key_query = """
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT CONCAT(id_1c_partner, '|', id_1c_contact)) as unique_composite_keys
            FROM partners
            """
            
            composite_result = db_client.execute_query(composite_key_query)
            if composite_result and len(composite_result) > 0:
                composite_stats = composite_result[0]
                
                if composite_stats['total_records'] != composite_stats['unique_composite_keys']:
                    logger.warning("‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –∫–ª—é—á–∏")
                    validation_status = "warning"
                
                stats.update(composite_stats)
            
            return {
                "status": validation_status,
                "partners": stats
            }
        else:
            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
            return {"status": "error", "message": "Validation failed"}
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}")
        raise

def generate_supplier_report_task(**context):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞–º"""
    try:
        from suppliers_etl.services.postgres.client import PostgresClient
        
        db_client = PostgresClient()
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç —Å–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        report_query = """
        SELECT 
            '–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞' as report_section,
            COUNT(*) as total_partners,
            COUNT(CASE WHEN COALESCE(is_client,false) = TRUE THEN 1 END) as total_clients,
            COUNT(CASE WHEN COALESCE(is_client,false) = TRUE THEN 1 END) as total_suppliers,
            COUNT(CASE WHEN COALESCE(is_client,false) = TRUE AND COALESCE(is_supplier,false) = TRUE THEN 1 END) as client_suppliers,
            NULL::integer as unique_roles,
            NULL::integer as managers,
            NULL::integer as directors,
            NULL::integer as other_roles
        FROM partners
        
        UNION ALL
        
        SELECT 
            '–ü–æ —Ä–æ–ª—è–º –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤' as report_section,
            NULL::integer as total_partners,
            NULL::integer as total_clients,
            NULL::integer as total_suppliers,
            NULL::integer as client_suppliers,
            COUNT(DISTINCT role) as unique_roles,
            COUNT(CASE WHEN role = '–ú–µ–Ω–µ–¥–∂–µ—Ä' THEN 1 END) as managers,
            COUNT(CASE WHEN role = '–î–∏—Ä–µ–∫—Ç–æ—Ä' THEN 1 END) as directors,
            COUNT(CASE WHEN role NOT IN ('–ú–µ–Ω–µ–¥–∂–µ—Ä', '–î–∏—Ä–µ–∫—Ç–æ—Ä') THEN 1 END) as other_roles
        FROM partners
        """
        
        result = db_client.execute_query(report_query)
        
        if result and len(result) > 0:
            return {"status": "success", "report_rows": len(result)}
        else:
            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á–µ—Ç")
            return {"status": "warning", "message": "Report generation failed"}
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}")
        raise

def final_summary_task(**context):
    """–§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è DAG"""
    try:
        logger.info("üéØ ========================================")
        logger.info("üéØ –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–í–û–î–ö–ê –í–´–ü–û–õ–ù–ï–ù–ò–Ø DAG")
        logger.info("üéØ ========================================")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –∑–∞–¥–∞—á
        ti = context['ti']
        extract_result = ti.xcom_pull(task_ids='extract_suppliers_data')
        load_result = ti.xcom_pull(task_ids='load_partners_to_postgres')
        validate_result = ti.xcom_pull(task_ids='validate_partners_data')
        report_result = ti.xcom_pull(task_ids='generate_supplier_report')
        
        logger.info("üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á:")
        logger.info(f"   ‚úÖ Extract: {len(extract_result) if extract_result else 0} –∑–∞–ø–∏—Å–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–æ")
        logger.info(f"   ‚úÖ Load: {load_result} –∑–∞–ø–∏—Å–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω–æ –≤ PostgreSQL")
        logger.info(f"   ‚úÖ Validate: –°—Ç–∞—Ç—É—Å –≤–∞–ª–∏–¥–∞—Ü–∏–∏ - {validate_result.get('status', 'unknown')}")
        logger.info(f"   ‚úÖ Report: –û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω - {report_result.get('status', 'unknown')}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
        all_success = all([
            extract_result is not None,
            load_result is not None,
            validate_result and validate_result.get('status') == 'success',
            report_result and report_result.get('status') == 'success'
        ])
        
        if all_success:
            logger.info("üéâ –í–°–ï –ó–ê–î–ê–ß–ò –í–´–ü–û–õ–ù–ï–ù–´ –£–°–ü–ï–®–ù–û!")
            logger.info("üéØ DAG SuppliersETL –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        else:
            logger.warning("‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–¥–∞—á–∏ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è–º–∏")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä–æ–∫—É
        logger.info("üéØ ========================================")
        
        return {
            "status": "success" if all_success else "warning",
            "summary": {
                "extract_count": len(extract_result) if extract_result else 0,
                "load_status": load_result,
                "validation_status": validate_result.get('status') if validate_result else 'unknown',
                "report_status": report_result.get('status') if report_result else 'unknown'
            }
        }
        
    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Å–≤–æ–¥–∫–∏: {str(e)}")
        raise

# –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–¥–∞—á —Å–æ–≥–ª–∞—Å–Ω–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
extract_operator = PythonOperator(
    task_id='extract_suppliers_data',
    python_callable=extract_suppliers_data_task,
    dag=dag
)

load_operator = PythonOperator(
    task_id='load_partners_to_postgres',
    python_callable=load_partners_to_postgres_task,
    dag=dag
)

validate_operator = PythonOperator(
    task_id='validate_partners_data',
    python_callable=validate_partners_data_task,
    dag=dag
)

report_operator = PythonOperator(
    task_id='generate_supplier_report',
    python_callable=generate_supplier_report_task,
    dag=dag
)

summary_operator = PythonOperator(
    task_id='final_summary',
    python_callable=final_summary_task,
    dag=dag
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–æ–≥–ª–∞—Å–Ω–æ –ø–æ—Ç–æ–∫—É –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:
# 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –ø–∞—Ä—Ç–Ω–µ—Ä–∞—Ö –∏–∑ Power BI —á–µ—Ä–µ–∑ DAX
# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü—É partners —Å –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–º –∫–ª—é—á–æ–º
# 3. –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
# 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞–º
# 5. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è DAG
extract_operator >> load_operator >> validate_operator >> report_operator >> summary_operator

if __name__ == "__main__":
    dag.cli()

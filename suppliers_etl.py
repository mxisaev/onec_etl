#!/usr/bin/env python3
"""
Suppliers ETL DAG
ETL Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°Ñ… (Ð¿Ð¾ÑÑ‚Ð°Ð²Ñ‰Ð¸ÐºÐ°Ñ… Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°Ñ…) Ð¸Ð· Power BI Ð² PostgreSQL
"""

import sys
import os
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable

# ðŸŽ¯ ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž: ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¿ÑƒÑ‚ÐµÐ¹ Ð´Ð»Ñ utils.logger
dags_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if dags_root not in sys.path:
    sys.path.insert(0, dags_root)

# ðŸŽ¯ Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ utils.logger
from utils.logger import get_logger

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ logger Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ DAG
logger = get_logger("suppliers_etl", "suppliers_etl")

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'SuppliersETL',
    default_args=default_args,
    description='ETL Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°Ñ… (Ð¿Ð¾ÑÑ‚Ð°Ð²Ñ‰Ð¸ÐºÐ°Ñ… Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°Ñ…) Ð¸Ð· Power BI Ð² PostgreSQL',
    schedule_interval='50 6,10 * * 1-5',  # ÐŸÐ½-ÐŸÑ‚ Ð² 6:50 Ð¸ 10:50 UTC (11:50 Ð¸ 15:50 UTC+5)
    catchup=False,
    tags=['etl', 'powerbi', 'postgres', 'suppliers', 'partners']
)

def extract_suppliers_data_task(**context):
    """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°Ñ… Ð¸Ð· Power BI Ñ‡ÐµÑ€ÐµÐ· DAX"""
    try:
        logger.info("ðŸ”„ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°Ñ… Ð¸Ð· Power BI...")
        
        from suppliers_etl.tasks.extract import extract_powerbi_data
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸ÑŽ dataset Ð¸Ð· Airflow Variables
        from airflow.models import Variable
        datasets = Variable.get('datasets', deserialize_json=True)
        partners_dataset = datasets['partners']
        
        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡Ð¸ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸
        task_config = {
            'dataset_id': partners_dataset['id'],  # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¸Ð· Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ð¾Ð¹ datasets
            'dax_query_key': 'partners',  # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐºÐ»ÑŽÑ‡ 'partners' Ð¸Ð· Airflow Variables
            'columns': {
                'Ð£Ð¢_ÐŸÐ°Ñ€Ñ‚Ð½ÐµÑ€Ñ‹[ÐŸÐ°Ñ€Ñ‚Ð½ÐµÑ€.Ð£Ð¢11]': 'partner',
                'Ð£Ð¢_ÐšÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½Ñ‹Ðµ Ð»Ð¸Ñ†Ð° Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð¾Ð²[ÐšÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½Ð¾Ðµ Ð»Ð¸Ñ†Ð¾]': 'contact',
                'Ð£Ð¢_ÐšÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½Ñ‹Ðµ Ð»Ð¸Ñ†Ð° Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð¾Ð²[email]': 'contact_email',
                'Ð£Ð¢_ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ð¸[_description]': 'responsible_manager',
                'Ð£Ð¢_ÐŸÐ°Ñ€Ñ‚Ð½ÐµÑ€Ñ‹[id_1c]': 'id_1c_partner',
                'Ð£Ð¢_ÐŸÐ°Ñ€Ñ‚Ð½ÐµÑ€Ñ‹[is_client]': 'is_client',
                'Ð£Ð¢_ÐŸÐ°Ñ€Ñ‚Ð½ÐµÑ€Ñ‹[is_supplier]': 'is_supplier',
                'Ð£Ð¢_ÐšÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½Ñ‹Ðµ Ð»Ð¸Ñ†Ð° Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð¾Ð²[Ð Ð¾Ð»ÑŒ]': 'role',
                'Ð£Ð¢_ÐšÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð½Ñ‹Ðµ Ð»Ð¸Ñ†Ð° Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð¾Ð²[id_1c]': 'id_1c_contact'
            }
        }
        
        result = extract_powerbi_data(task_config)
        return result
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°Ñ…: {str(e)}")
        raise

def load_partners_to_postgres_task(**context):
    """Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°Ñ… Ð² PostgreSQL"""
    try:
        logger.info("ðŸ”„ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² PostgreSQL...")
        
        # Ð“Ð¾Ñ€ÑÑ‡Ð°Ñ Ð¿ÐµÑ€ÐµÐ·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¼Ð¾Ð´ÑƒÐ»ÐµÐ¹, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð²Ð¾Ñ€ÐºÐµÑ€ Ð¿Ð¾Ð´Ñ†ÐµÐ¿Ð¸Ð» Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ PostgresClient/merge
        import importlib
        import suppliers_etl.services.postgres.client as pg_client
        import suppliers_etl.tasks.load as load_module
        importlib.reload(pg_client)
        importlib.reload(load_module)
        execute_etl_task = load_module.execute_etl_task
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰ÐµÐ¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸
        ti = context['ti']
        data = ti.xcom_pull(task_ids='extract_suppliers_data')
        
        if data is None:
            raise ValueError("ÐÐµÑ‚ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°Ñ… Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸")
        
        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² DataFrame
        import pandas as pd
        df = pd.DataFrame(data)
        
        logger.info(f"ðŸ“Š DataFrame ÑÐ¾Ð·Ð´Ð°Ð½: {len(df)} ÑÑ‚Ñ€Ð¾Ðº, {len(df.columns)} ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº")
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ† Ð´Ð»Ñ PostgreSQL
        from airflow.models import Variable
        datasets = Variable.get('datasets', deserialize_json=True)
        
        # ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸
        task_config = {
            'source_table': datasets['partners']['source_table'],
            'target_table': datasets['partners']['target_table'],
            'mapping_name': 'partners'
        }
        
        # ÐŸÑ€Ð¾Ð²Ð¾Ð´Ð¸Ð¼ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ Ñ‡ÐµÑ€ÐµÐ· PostgresClient, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð³Ð°Ñ€Ð°Ð½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ MERGE
        from suppliers_etl.config.dax_mappings import get_dax_mapping
        mapping = get_dax_mapping(task_config['mapping_name'])
        
        # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
        df['is_vector'] = False
        df = df.rename(columns=mapping['columns'])
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð±ÐµÐ· Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð±Ð¸Ð·Ð½ÐµÑ-ÐºÐ»ÑŽÑ‡Ð° id_1c_partner
        before = len(df)
        df = df.dropna(subset=['id_1c_partner'])
        df = df[df['id_1c_partner'].astype(str).str.strip() != '']
        dropped = before - len(df)
        if dropped > 0:
            logger.info(f"ðŸ§¹ ÐžÑ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð¾ Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð±ÐµÐ· id_1c_partner: {dropped}")
        key_columns = ['id_1c_partner', 'id_1c_contact']
        
        # ÐšÐ¾Ð»Ð¾Ð½ÐºÐ¸ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹
        technical_fields = {'partner_uid', 'id_1c_partner', 'id_1c_contact', 'is_vector', 'created_at', 'updated_at', 'vector', 'extracted_at'}
        columns_for_change_analysis = [col for col in df.columns if col not in technical_fields]
        
        # Ð¢Ð¸Ð¿Ñ‹ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
        def get_column_type(col_name):
            if col_name in ['partner_uid']:
                return 'UUID'
            elif col_name in ['id_1c_partner', 'id_1c_contact']:
                return 'VARCHAR'
            elif col_name in ['is_client', 'is_supplier', 'is_vector']:
                return 'BOOLEAN'
            else:
                return 'TEXT'
        
        PostgresClient = pg_client.PostgresClient
        client = PostgresClient()
        result = client.merge_data(
            table_name=task_config['target_table'],
            data=df,
            key_columns=key_columns,
            columns=[{'name': col, 'dataType': get_column_type(col)} for col in df.columns],
            template_name=mapping['table_template'],
            columns_for_change_analysis=columns_for_change_analysis
        )
        
        return result
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°Ñ…: {str(e)}")
        raise

def validate_partners_data_task(**context):
    """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°Ñ…"""
    try:
        from suppliers_etl.services.postgres.client import PostgresClient
        
        db_client = PostgresClient()
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð¾Ð²
        partners_query = """
        SELECT 
            COUNT(*) as total_partners,
            COUNT(CASE WHEN partner IS NOT NULL THEN 1 END) as partners_with_name,
            COUNT(CASE WHEN contact IS NOT NULL THEN 1 END) as partners_with_contact,
            COUNT(CASE WHEN contact_email IS NOT NULL THEN 1 END) as partners_with_email,
            COUNT(CASE WHEN responsible_manager IS NOT NULL THEN 1 END) as partners_with_manager,
            COUNT(CASE WHEN is_client = TRUE THEN 1 END) as total_clients,
            COUNT(CASE WHEN is_supplier = TRUE THEN 1 END) as total_suppliers
        FROM partners
        """
        
        result = db_client.execute_query(partners_query)
        
        if result and len(result) > 0:
            stats = result[0]
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ñ…
            validation_status = "success"
            if stats['total_partners'] == 0:
                validation_status = "error"
            elif stats['partners_with_name'] == 0:
                validation_status = "warning"
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ‚Ð½Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡
            composite_key_query = """
            SELECT 
                COUNT(*) as total_records,
                COUNT(DISTINCT CONCAT(id_1c_partner, '|', id_1c_contact)) as unique_composite_keys
            FROM partners
            """
            
            composite_result = db_client.execute_query(composite_key_query)
            if composite_result and len(composite_result) > 0:
                composite_stats = composite_result[0]
                
                if composite_stats['total_records'] != composite_stats['unique_composite_keys']:
                    logger.warning("âš ï¸ ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð´ÑƒÐ±Ð»Ð¸Ñ€ÑƒÑŽÑ‰Ð¸ÐµÑÑ ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ‚Ð½Ñ‹Ðµ ÐºÐ»ÑŽÑ‡Ð¸")
                    validation_status = "warning"
                
                stats.update(composite_stats)
            
            return {
                "status": validation_status,
                "partners": stats
            }
        else:
            logger.warning("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸")
            return {"status": "error", "message": "Validation failed"}
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸: {str(e)}")
        raise

def generate_supplier_report_task(**context):
    """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° Ð¿Ð¾ Ð¿Ð¾ÑÑ‚Ð°Ð²Ñ‰Ð¸ÐºÐ°Ð¼"""
    try:
        from suppliers_etl.services.postgres.client import PostgresClient
        
        db_client = PostgresClient()
        
        # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ñ‚Ñ‡ÐµÑ‚ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸
        report_query = """
        SELECT 
            'ÐžÐ±Ñ‰Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°' as report_section,
            COUNT(*) as total_partners,
            COUNT(CASE WHEN COALESCE(is_client,false) = TRUE THEN 1 END) as total_clients,
            COUNT(CASE WHEN COALESCE(is_client,false) = TRUE THEN 1 END) as total_suppliers,
            COUNT(CASE WHEN COALESCE(is_client,false) = TRUE AND COALESCE(is_supplier,false) = TRUE THEN 1 END) as client_suppliers,
            NULL::integer as unique_roles,
            NULL::integer as managers,
            NULL::integer as directors,
            NULL::integer as other_roles
        FROM partners
        
        UNION ALL
        
        SELECT 
            'ÐŸÐ¾ Ñ€Ð¾Ð»ÑÐ¼ ÐºÐ¾Ð½Ñ‚Ð°ÐºÑ‚Ð¾Ð²' as report_section,
            NULL::integer as total_partners,
            NULL::integer as total_clients,
            NULL::integer as total_suppliers,
            NULL::integer as client_suppliers,
            COUNT(DISTINCT role) as unique_roles,
            COUNT(CASE WHEN role = 'ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€' THEN 1 END) as managers,
            COUNT(CASE WHEN role = 'Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€' THEN 1 END) as directors,
            COUNT(CASE WHEN role NOT IN ('ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€', 'Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€') THEN 1 END) as other_roles
        FROM partners
        """
        
        result = db_client.execute_query(report_query)
        
        if result and len(result) > 0:
            return {"status": "success", "report_rows": len(result)}
        else:
            logger.warning("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ñ‡ÐµÑ‚")
            return {"status": "warning", "message": "Report generation failed"}
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°: {str(e)}")
        raise

def final_summary_task(**context):
    """Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ DAG"""
    try:
        logger.info("ðŸŽ¯ ========================================")
        logger.info("ðŸŽ¯ Ð¤Ð˜ÐÐÐ›Ð¬ÐÐÐ¯ Ð¡Ð’ÐžÐ”ÐšÐ Ð’Ð«ÐŸÐžÐ›ÐÐ•ÐÐ˜Ð¯ DAG")
        logger.info("ðŸŽ¯ ========================================")
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð²ÑÐµÑ… Ð·Ð°Ð´Ð°Ñ‡
        ti = context['ti']
        extract_result = ti.xcom_pull(task_ids='extract_suppliers_data')
        load_result = ti.xcom_pull(task_ids='load_partners_to_postgres')
        validate_result = ti.xcom_pull(task_ids='validate_partners_data')
        report_result = ti.xcom_pull(task_ids='generate_supplier_report')
        
        logger.info("ðŸ“‹ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡:")
        logger.info(f"   âœ… Extract: {len(extract_result) if extract_result else 0} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¾")
        logger.info(f"   âœ… Load: {load_result} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ Ð² PostgreSQL")
        logger.info(f"   âœ… Validate: Ð¡Ñ‚Ð°Ñ‚ÑƒÑ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ - {validate_result.get('status', 'unknown')}")
        logger.info(f"   âœ… Report: ÐžÑ‚Ñ‡ÐµÑ‚ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½ - {report_result.get('status', 'unknown')}")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð¾Ð±Ñ‰Ð¸Ð¹ ÑÑ‚Ð°Ñ‚ÑƒÑ
        all_success = all([
            extract_result is not None,
            load_result is not None,
            validate_result and validate_result.get('status') == 'success',
            report_result and report_result.get('status') == 'success'
        ])
        
        if all_success:
            logger.info("ðŸŽ‰ Ð’Ð¡Ð• Ð—ÐÐ”ÐÐ§Ð˜ Ð’Ð«ÐŸÐžÐ›ÐÐ•ÐÐ« Ð£Ð¡ÐŸÐ•Ð¨ÐÐž!")
            logger.info("ðŸŽ¯ DAG SuppliersETL Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾")
        else:
            logger.warning("âš ï¸ ÐÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ð»Ð¸ÑÑŒ Ñ Ð¿Ñ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸ÑÐ¼Ð¸")
        
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ
        logger.info("ðŸŽ¯ ========================================")
        
        return {
            "status": "success" if all_success else "warning",
            "summary": {
                "extract_count": len(extract_result) if extract_result else 0,
                "load_status": load_result,
                "validation_status": validate_result.get('status') if validate_result else 'unknown',
                "report_status": report_result.get('status') if report_result else 'unknown'
            }
        }
        
    except Exception as e:
        logger.exception(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÑÐ²Ð¾Ð´ÐºÐ¸: {str(e)}")
        raise

# Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ðµ Ð¸Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸
extract_operator = PythonOperator(
    task_id='extract_suppliers_data',
    python_callable=extract_suppliers_data_task,
    dag=dag
)

load_operator = PythonOperator(
    task_id='load_partners_to_postgres',
    python_callable=load_partners_to_postgres_task,
    dag=dag
)

validate_operator = PythonOperator(
    task_id='validate_partners_data',
    python_callable=validate_partners_data_task,
    dag=dag
)

report_operator = PythonOperator(
    task_id='generate_supplier_report',
    python_callable=generate_supplier_report_task,
    dag=dag
)

summary_operator = PythonOperator(
    task_id='final_summary',
    python_callable=final_summary_task,
    dag=dag
)

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹ ÑÐ¾Ð³Ð»Ð°ÑÐ½Ð¾ Ð¿Ð¾Ñ‚Ð¾ÐºÑƒ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸:
# 1. Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾ Ð¿Ð°Ñ€Ñ‚Ð½ÐµÑ€Ð°Ñ… Ð¸Ð· Power BI Ñ‡ÐµÑ€ÐµÐ· DAX
# 2. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð² Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ partners Ñ ÐºÐ¾Ð¼Ð¿Ð¾Ð·Ð¸Ñ‚Ð½Ñ‹Ð¼ ÐºÐ»ÑŽÑ‡Ð¾Ð¼
# 3. Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
# 4. Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° Ð¿Ð¾ Ð¿Ð¾ÑÑ‚Ð°Ð²Ñ‰Ð¸ÐºÐ°Ð¼
# 5. Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÑÐ²Ð¾Ð´ÐºÐ° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ DAG
extract_operator >> load_operator >> validate_operator >> report_operator >> summary_operator

if __name__ == "__main__":
    dag.cli()
